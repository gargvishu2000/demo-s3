= Installation Guide - Portal
Active Intelligence Platform
:title-logo-image: image:/theme/logo.png[pdfwidth=3.5in,align=right]
:revnumber: v0.1.1
:doctype: book
:encoding: utf-8
:lang: en
:numbered:
:icons: font
:source-highlighter: rouge
:sectnumlevels: 5
:toc: left
:chapter-label:


== Introduction

This document details the portal installation.

=== Naming Conventions

In this document the following naming conventions will be established.

|===
|Variable | Meaning/mapping

|${YOUR_NAMESPACE} | The kubernetes namespace where to deploy all the elements of the installation
|${UPGRADE_CHART_VERSION} | The chart version to be used when upgrading the deployment
|${UPGRADE_IMAGE_REG} | The new image registry to be used when upgrading the deployment
|${UPGRADE_IMAGE_REPO} | The new image repository to be used when upgrading the deployment
|${UPGRADE_IMAGE_TAG} | The new image tag to be used when upgrading the deployment

|===

[#_installation_steps]
== Installation Steps

The steps below assume all the third parties and tools are ready and that the images are already loaded on your image repository.

=== Step 1 - Add your own settings

Settings can be added in two ways: *inline* or *file*

==== Inline settings

Inline settings can be set using the `--set` option, for instance, to set what storage type we will be using we can add the following options:

[source,shell]
----
--set storage.type=s3
----

==== Alternative of object storage

If object storage is not available then create data,process and resources volumes

[source,bash]
helm install \
  --set storage.type=local \
  --set persistence.data.enabled=true \
  --set persistence.data.createPV=true \
  --set persistence.data.createPVC=true \
  --set persistence.data.hostPath="/opt/portal/data/" \
  --set persistence.data.storageClass="manual" \
  --set persistence.process.enabled=true \
  --set persistence.process.createPV=true \
  --set persistence.process.createPVC=true \
  --set persistence.process.hostPath="/opt/portal/process/" \
  --set persistence.process.storageClass="manual" \
  --set persistence.resources.enabled=true \
  --set persistence.resources.createPV=true \
  --set persistence.resources.createPVC=true \
  --set persistence.resources.hostPath="/opt/portal/resources/" \
  --set persistence.resources.storageClass="manual" mydeployment ...

Going through the settings:

- *persistence.data.enabled*: Enables the usage of shared volumes

- *persistence.data.createPV*: Creates and PersistentVolume. You can specify the size, accessModes and hostPath.

- *persistence.data.createPVC*: Creates and PersistentVolumeClaim. You can specify the size, accessModes and hostPath.

- *persistence.process.enabled*: Enables the usage of shared volumes

- *persistence.process.createPV*: Creates and PersistentVolume. You can specify the size, accessModes and hostPath.

- *persistence.process.createPVC*: Creates and PersistentVolumeClaim. You can specify the size, accessModes and hostPath.

- *persistence.resources.enabled*: Enables the usage of shared volumes

- *persistence.resources.createPV*: Creates and PersistentVolume. You can specify the size, accessModes and hostPath.

- *persistence.resources.createPVC*: Creates and PersistentVolumeClaim. You can specify the size, accessModes and storageClass.

==== File settings [[filesettings]]

File settings are a good way to share settings across installations.Typically, this is a good option for settings like storage access.To start using settings through files, first you need to create a `yaml` file which contains your settings.The following snippet shows how can we add storage settings in a file as well as some shared configurations between all installations.

[source,yaml]
----
deployment:
  base-dir: s3://mybucket/example
storage:
  type: s3
  s3:
    endpoint: http://localhost:9001
    credentials-type: static
    access-key-id: myaccesskey
    secret-access-key: mysecretkey
----

==== Prepare TLS certificates ( Enable HTTPS mode)

To add certificates to the images we need a truststore generated. This truststore must be named as
keystore.jks. The truststore can be used as secret.


===== Create trust store if not exists or use if already exists

  Note: update -alias and CN name according to the domain

[source,bash]
----
keytool -genkey -keyalg rsa -keysize 2048 -validity 3650 -keypass roamware -alias  "raidssl.mobileum.com"  -storepass roamware -keystore keystore.jks -deststoretype pkcs12 -dname "CN=raidssl.mobileum.com"
----

===== Create Secret for keystore.jks

[source,bash]
----
kubectl create secret generic keystore-secret-name --from-file=keystore.jks
----

===== Insert or update following records in product-config.xml:

  Note: user should use KeyStorePath as /var/keystore/keystore.jks

[source,shell]
----
<boolean name="WebServerUseSSL">true</boolean>
<string name="KeyStorePath">/var/keystore/keystore.jks</string>
<string name="KeyStorePwd">roamware</string>
<string name="KeyStoreAlias">raidssl.mobileum.com</string>
----

===== set secret names as follows:

[source,shell]
----
--set tls.enabled=true
--set tls.secret=keystore-secret-name
----

==== Prepare product-config.xml and license.lic

To setup `product-config.xml` and `license.lic`,  create secret for both items and pass the secret names in parameters list or directly upload them on s3 bucket or gs bucket.

Keep `license.lic` and `product-config.xml` prepared.

[source,bash]
----
kubectl create secret generic portal-license --from-file=license.lic
kubectl create secret generic portal-product-config --from-file=product-config.xml
----

set secret names as follows:

[source,shell]
----
--set deployment.productConfig.secretName=portal-product-config
--set deployment.license.secretName=portal-license
----

You can check all available settings <<configuration, here>>.

=== Step 2 - Prepare your installation

Now that your have all images loaded and required settings in place, before start installing, you first need to upload your product configurations to the storage you are using (s3, gs, ...).

[WARNING]
Uploaded files coming from setup processes should be placed under a folder with the *exact same name as the deployment*

This process is composed of two steps: *setup* and *startup* process.
Each one of these processes requires a different storage location under the *deployment directory*.

[NOTE]
The deployment directory is located under `deployment.base-dir` with the same name of your deployment given by `deployment.name`.
For instance, with the settings described <<filesettings, here>>, if your `deployment.name` is `rafm` then your deployment directory would be `s3://mybucket/example/rafm`

There are four different directories where you can get/upload data from/to.
All these directories are located under deployment directory.

- `baseDir`: This directory represents the location on the remote storage where you have uploaded your setup/startup configuration.
For instance, if your `deployment.setup-path` is `mysetup` then you should upload your setup files into `s3://mybucket/example/rafm/mysetup`.
Same applies for startup process but using `deployment.startup-path`.
It will be *loaded when the process starts*.
- `dataDir`: Directory that holds data specific to a deployment.
This directory is located under in the deployment directory with the name `data` and it will only be loaded by the deployment itself.
It will be *loaded when the process* starts and all its content will be *uploaded to storage when it finishes*.
- `shareDir`: This directory allows other deployments to upload data from their setup.
It will be *loaded when the process starts*.
- `targetShareDir`: This directory points to other deployment `shares` directory.
All its content will be *uploaded to that deployment `shares` directory when the process finishes*.
To set what deployment you want to share data with, use the `deployment.upload-target` option.

[WARNING]
Make sure to never use the name `data` or `shares` for your setup or startup directories.

==== Setup [[setup]]

This is launched before the installation and will prepare your environment with all requirements that your server needs.
This is usually used to things like create tables in a database for the first time.

At a high level, this process will create an instance that will prepare RAFM database, packs its registry to be reused on startup and uploads web resources to other server's shared location.
Keep in mind that you need to *have all the files available at your setup directory*.

[WARNING]
This process will execute every time you run `install` commands.
To ignore it after the first installation use the option `--no-hooks`

[NOTE]
Since the setup process can take a while, we advise you to increase the installation timeout using the option `--timeout 30m` (example for a 30-minute timeout)

===== Using storage

You can upload all required setup content into a remote storage, under your *setup* location that can be set using the option `deployment.setup-path`.

===== Extending base image

You can *extend the base image* with your own custom `setup` process. This can be helpful when you don't have access to a storage, so you add your custom process to the image instead.

To extend a portal image and add your `setup` process, your can use the following template:

[souces,dockerfile]
----
FROM aip/portal-8.2.5_220930:0.5.0

COPY myCustomSetup/wpkg.yaml /opt/extension/baseline/setup/
----

==== Startup [[startup]]

This process happens everytime you boot a server.
It has the ability to reuse data from setup processes and shared data that can be delivered to this deployment by other deployments.

===== Using storage

You can upload all required startup content into a remote storage, under your *startup* location that can be set using the option `deployment.startup-path`.
===== Extending base image

You can *extend the base image* with your own custom `startup` process. This can be helpful when you don't have access to a storage, so you add your custom process to the image instead.

To extend a portal image and add your `startup` process, your can use the following template:

[souces,dockerfile]
----
FROM aip/portal-8.2.5_220930:0.5.0

COPY myCustomStartup/wpkg.yaml /opt/extension/baseline/startup/
----

=== Step 3 - Install your server

To install any server you will need the correspondent chart.
All available charts can be found under `charts` directory of distribution package.
For example, to install a RAFM server you would run the following command:

[source,bash]
----
helm install portal \
  --set deployment.name=portal \
  --set deployment.setup-path=setup \
  --set deployment.startup-path=startup
  -f mysettings.yaml \
  --timeout 30m \
  <PATH_TO_BUNDLE>/charts/
----

=== Step 4 - Restart the portal to receive new WAR files

Whenever you add new modules you'll need to restart portal so that it can detect the resources (e.g WAR files and onlinehelp).

[source,bash]
----
kubectl scale deployment portal --replicas=0 && \
kubectl scale deployment portal --replicas=1
----

== Uninstall steps

[WARNING]
This process will execute a cleanup process, *cleaning up all database objects*, and then removes the deployment from kubernetes.
To ignore the database cleanup, and remove only the deployment from kubernetes, use the option `--no-hooks`.

=== Step 1 - Cleanup

This process is structurally similar to the <<setup, setup>> and <<startup, startup>>.

==== Using storage

You can upload all required cleanup content into a remote storage, under your *cleanup* location that can be set using the option `deployment.cleanup-path`.

==== Extending base image

You can *extend the base image* with your own custom `cleanup` process. This can be helpful when you don't have access to a storage, so you add your custom process to the image instead.

To extend a portal image and add your `cleanup` process, your can use the following template:

[souces,dockerfile]
----
FROM aip/portal-8.2.5_220930:0.5.0

COPY myCustomCleanup/wpkg.yaml /opt/extension/baseline/cleanup/
----

=== Step 2 - Uninstall

To uninstall a deployment, run the following command:

[source,bash]
----
helm uninstall portal \
  --set deployment.cleanup-path=cleanup \
  -f mysettings.yaml \
  --timeout 30m
----

=== Step 3 - Storage cleanup

The uninstallation process does not affect the storage, so this step needs to be done manually.
You can achieve this by simply removing the contents of the directories:
- `dataDir`
- `shareDir`
- `targetShareDir`

== Upgrade steps

The upgrade scenario is structurally similar to the <<setup, setup>> process.

=== Step 1 - Configuration

==== Using storage

[#_upgrade_path]
You can upload all required upgrade content into a remote storage, under your *upgrade* location that can be set using the option `deployment.upgrade-path`.

==== Extending base image

You can *extend the base image* with your own custom `upgrade` process. This can be helpful when you don't have access to a storage, so you add your custom process to the image instead.

To extend a portal image and add your `upgrade` process, your can use the following template:

[souces,dockerfile]
----
FROM aip/portal-8.2.5_220930:0.5.0

COPY myCustomUpgrade/wpkg.yaml /opt/extension/baseline/upgrade/
----

=== Step 1 - Upgrade deployment

Upgrade the deployment, running the following command:

[source,bash]
----
helm upgrade portal \
  --set deployment.upgrade-path=upgrade \
  --set image.registry=${UPGRADE_IMAGE_REG} \
  --set image.repository=${UPGRADE_IMAGE_REPO} \
  --set image.tag=${UPGRADE_IMAGE_TAG} \
  --timeout 30m \
  <PATH_TO_BUNDLE>/charts/ \
  --version "${UPGRADE_CHART_VERSION}"
----

After the upgrade job completes, the old portal pod will be removed, and the new one with the upgraded image will spawn.

== SSO

=== Installation

To add SSO to your portal server you need to have the directory `plugins` in your startup directory with the `sso.war` file.
Then the WPKG of startup will copy the file to portal pod and it will deploy the SSO.

=== Configuration

You need to configure your SSO through the portal configuration file (product-config.xml), with the CustomPortalProperties parameter, as described in SSO Configuration manual.

== Overlay

If you want to extend the baseline image with new settings, for instance, external libraries, SSO configuration files, you should have one directory named `overlay` inside your deployment startup folder.

There you can put all the files and directories you want and in startup process they will be copied into the image instance directory.

== Patches

=== Setup and Upgrade

Setup and Upgrade patches can be applied in two ways:

* *Before instance is created*: All patches should be placed under `pre-patches`.
This patches can be used to change any existing software configuration before the server is created.
* *After instance is created*: This type of patches will execute once server is already created and all configurations are stored on database.
To use this, add all patches under `post-patches`.

[NOTE]
`pre-patches` and `post-patches` are located under your deployment setup or upgrade location.

=== Startup

You can automatically apply product patches by placing all official patches under `patches` of your deployment startup directory.

[NOTE]
All patches will be applied before server starts on each deployment pod.

[WARNING]
Patches that imply changes of database or that can only be executed once should not be included in this process

=== SPO, JavaScript and Database changes

==== Step 1 - Configuration

===== Using storage

Patches with changes to the database, JavaScript and SPO patches behave the same way: a `patch` process, that requires a storage location under the *deployment directory*, and that can be set using the option `deployment.patch-path`.

At a high level, this process will launch a job to apply all patches inside `patch/patches` under the *deployment directory*.

[NOTE]
All patches should be placed under `patches` of your deployment patch directory.

===== Extending base image

You can *extend the base image* with your own custom `patch` process. This can be helpful when you don't have access to a storage, so you add your custom process to the image instead.

To extend a portal image and add your `patch` process, your can use the following template:

[souces,dockerfile]
----
FROM aip/portal-8.2.5_220930:0.5.0

COPY myCustomPatch/wpkg.yaml /opt/extension/baseline/patch/
----

==== Step 2 - Apply patch

To launch the *patch process*, run the following command:

[source,bash]
----
helm upgrade portal \
  --set deployment.patch-path=patch \
  --set image.registry=${UPGRADE_IMAGE_REG} \
  --set image.repository=${UPGRADE_IMAGE_REPO} \
  --set image.tag=${UPGRADE_IMAGE_TAG} \
  --timeout 30m \
  <PATH_TO_BUNDLE>/charts/ \
  --version "${UPGRADE_CHART_VERSION}"
----

[NOTE]
With the settings described <<filesettings, here>>, all patches inside `s3://mybucket/example/portal/patch/patches` will be applied.

==== Step 3 - Restart the portal to receive new WAR files

When applying this type of patches you'll need to restart portal so that it can detect the new resources.

[source,bash]
----
kubectl scale deployment portal --replicas=0 && \
kubectl scale deployment portal --replicas=1
----

== Maintenance

=== Changing configuration on `product-config.xml`

To change any server configuration on already running servers follow the next steps

==== Step 1 - Edit `product-config.xml`

First, you need to edit and add your new configurations.
To do it, go to your deployment startup directory and download the `product-config.xml` that you've uploaded previously.
For instance, with the settings described <<filesettings, here>>, if your `deployment.name` is `rafm` you would use the following command:

[source,shell]
----
$ aws s3 cp s3://mybucket/example/rafm/startup/product-config.xml .
----

After changing the `product-config.xml` with all new changes, upload it back to the same location:
[source,shell]

----
$ aws s3 cp product-config.xml s3://mybucket/example/rafm/startup/product-config.xml
----

==== Step 2 - Restart your deployment

Now your servers are ready to read back the new configurations and apply them.
To do this, restart your deployment by executing the following command:

[source,bash]
----
kubectl scale deployment rafm --replicas=0 && \
kubectl scale deployment rafm --replicas=1
----

=== Logging

Server logs are written using a json format. This format contains information about specific areas of RAFM product. This tag, `context`, can be displayed with following values: `catalina` or `audit`. Value in place are a direct mapping to previous log files.

== Configuration [[configuration]]

All configuration is done either using the inline approach or a yaml file which is expected to be a flat collection of YAML key value pairs with format key: value.

=== Image

[cols="2,1,1,4"]
|===
|Key|Default|Type|Description

|image.pullPolicy
|IfNotPresent
|String
|Kubernetes image pull policy.

|image.tag
|
|String
|The image tag

|image.registry
|
|String
|Registry where image is located

|image.repository
|
|String
|Repository where image is located

|volumePermissionsImage.pullPolicy
|IfNotPresent
|String
|Kubernetes image pull policy for the image used on init container to adjust volume permissions.

|volumePermissionsImage.tag
|0.2.2
|String
|The image tag

|volumePermissionsImage.registry
|aip/base
|String
|Registry where image used on init container to adjust volume permissions is located.

|volumePermissionsImage.repository
|
|String
|Repository where image used on init container to adjust volume permissions is located.
|===

=== Storage

[cols="2,1,1,4"]
|===
|Key|Default|Type|Description

|storage.type
|local
|String
a|The filesystem type to support all operations. Possible values: *s3*, *gs*

|===

==== S3

[cols="2,1,1,4"]
|===
|Key|Default|Type|Description

|storage.s3.endpoint
|http://s3.amazonaws.com
|String
|The AWS store’s connection URL

|storage.s3.region
|us-east-1
|String
|The AWS region to be used the connector.

|storage.s3.credentials-type
|default
|String
|Configure the credentials provider that should be used to authenticate with AWS.

|storage.s3.access-key-id
|(none)
|String
|The AWS access key ID used to authenticate personal AWS credentials such as IAM credentials.

|storage.s3.secret-access-key
|(none)
|String
|The secret access key used to authenticate personal AWS credentials such as IAM credentials.
|===

==== Google cloud storage

[cols="2,1,1,4"]
|===
|Key|Default|Type|Description

|storage.gs.project-id
|
|String
|The account project id

|storage.gs.service-account-encoded-key
|
|String
|The service account base64 encoded content.
|===

=== Deployment

[cols="2,1,1,4"]
|===
|Key|Default|Type|Description

|deployment.name
|(none)
|String
|The deployment name to use.

[#_base_dir]
|deployment.base-dir
|(none)
|String
|The directory used to store deployments and the result of deployment setups. This takes a path URI like s3://mybucket/example.

|deployment.productConfig.secretName
|(none)
|String
|This accepts secret name which contains product-config.xml file.

|deployment.license.secretName
|(none)
|String
|This accepts secret name which contains license.lic file.

|deployment.setup-path
|setup
|String
|The directory used to store a deployment setup process. Relative path to the `deployment.base-dir` property.

|deployment.startup-path
|startup
|String
|The directory used to store a deployment startup process. Relative path to the `deployment.base-dir` property.

|deployment.cleanup-path
|cleanup
|String
|The directory used to store a deployment cleanup process. Relative path to the `deployment.base-dir` property.

|deployment.upgrade-path
|upgrade
|String
|The directory used to store a deployment upgrade process. Relative path to the `deployment.base-dir` property.

|deployment.patch-path
|patch
|String
|The directory used to store a deployment patch process. Relative path to the `deployment.base-dir` property.
|===

=== Probes

==== Liveness

[cols="2,1,1,4"]
|===
|Key|Default|Type|Description

|deployment.livenessProbe.initialDelaySeconds
|120
|Integer
|Number of seconds after the container has started before liveness initiated. Minimum value is 0.

|deployment.livenessProbe.periodSeconds
|60
|Integer
|How often (in seconds) to perform the probe. Minimum value is 1.

|deployment.livenessProbe.timeoutSeconds
|30
|Integer
|Number of seconds after which the probe times out. Minimum value is 1.

|deployment.livenessProbe.failureThreshold
|3
|Integer
|When a probe fails, Kubernetes will try failureThreshold times before giving up. Giving up means restarting the container. Minimum value is 1.

|deployment.livenessProbe.successThreshold
|1
|String
|Minimum consecutive successes for the probe to be considered successful after having failed. Minimum value is 1.

|===

==== Readiness

[cols="2,1,1,4"]
|===
|Key|Default|Type|Description

|deployment.readinessProbe.initialDelaySeconds
|120
|Integer
|Number of seconds after the container has started before liveness initiated. Minimum value is 0.

|deployment.readinessProbe.periodSeconds
|60
|Integer
|How often (in seconds) to perform the probe. Minimum value is 1.

|deployment.readinessProbe.timeoutSeconds
|30
|Integer
|Number of seconds after which the probe times out. Minimum value is 1.

|deployment.readinessProbe.failureThreshold
|5
|Integer
|When a probe fails, Kubernetes will try failureThreshold times before giving up. Giving up means the container will be marked Unready. Minimum value is 1.

|deployment.readinessProbe.successThreshold
|1
|String
|Minimum consecutive successes for the probe to be considered successful after having failed. Minimum value is 1.

|===

=== Resources

==== Deployment

[cols="2,1,1,4"]
|===
|Key|Default|Type|Description

|deployment.resources.requests.memory
|
|String
|The requested memory resource for the deployment container. According to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#resource-units-in-kubernetes[resource units]

|deployment.resources.requests.cpu
|
|String
|The requested CPU resource for the deployment container. According to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#resource-units-in-kubernetes[resource units]

|deployment.resources.limits.memory
|
|String
|The limit memory resource that a deployment container can use. According to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#resource-units-in-kubernetes[resource units]

|deployment.resources.limits.cpu
|
|String
|The limit cpu resource that a deployment container can use. According to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#resource-units-in-kubernetes[resource units]

|===

==== Setup

[cols="2,1,1,4"]
|===
|Key|Default|Type|Description

|setup.resources.requests.memory
|
|String
|The requested memory resource for the container responsible for setting up a deployment environment. According to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#resource-units-in-kubernetes[resource units]

|setup.resources.requests.cpu
|
|String
|The requested CPU resource for the container responsible for setting up a deployment environment. According to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#resource-units-in-kubernetes[resource units]

|setup.resources.limits.memory
|
|String
|The limit memory resource that a setup process can use. According to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#resource-units-in-kubernetes[resource units]

|setup.resources.limits.cpu
|
|String
|The limit cpu resource that a setup process can use. According to https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#resource-units-in-kubernetes[resource units]

|===

=== Certificates

[cols="2,1,1,4"]
|===
|Key|Default|Type|Description

|cacerts.configmap
|(none)
|String
|The name of the configmap that handles with certificates.

|cacerts.secret
|(none)
|String
|The name of the secret that handles with certificates.

|===

=== Environment variables

[cols="2,1,1,4"]
|===
|Key|Default|Type|Description

|global.envs
|(none)
|Map
|A key value configuration that enables you to add custom environment variables to your containers.

|===

=== Logging

[cols="2,1,1,4"]
|===
|Key|Default|Type|Description

|deployment.logLevel
|info
|String
|Log level for portal server. Can be trace, debug, info, warn, error or fatal

|deployment.auditLogLevel
|info
|String
|Log level for audit actions happening on the server. Can be trace, debug, info, warn, error or fatal

|===

== Troubleshooting

=== How to overcome timeouts during job installation?

Some environments might take more time to install than others.
If this happens, when executing `install` command, you'll find a similar message to the one below:

[source,shell]
----
Error: failed pre-install: timed out waiting for the condition
----

This means that the setup process is taking longer than expected and will prevent your deployment to continue its normal flow.

:sectnums!:
==== Step 1: Wait until setup job finishes

Keep watching the pod that is executing the job and wait it gets into `Completed` state.

==== Step 2: Uninstall your previous deployment

In order to resume your deployment you'll need to uninstall it first.
When running the `uninstall` command, don't select the option to clean your environment (type `n`).

==== Step 3: Repeat installation step

Now you can resume your installation by executing the same command that you've used previously but this time with a new property, `--no-hooks`.

:sectnums:

=== BackoffLimitExceed error during installation

This type of error happens when the setup process fails.
When this happens you'll see an error similar to the one below:

[source,shell]
----
Error: failed pre-install: job failed: BackoffLimitExceeded
----

To check what happened, check the setup process logs.
After, `uninstall` your deployment and `install` it again.

=== Environment cleanup

==== Standard cleanup

Uninstall Portal

[source,shell]
----
$ /bin/deploymentmanager.sh uninstall portal --wait
----

Delete jobs

[source,shell]
----
$ kubectl delete jobs `kubectl get jobs -o custom-columns=:.metadata.name`
----

Delete S3 resources
[source,shell]

----
$  aws s3 rm --recursive s3://bucket-name/portal/registry
$  aws s3 rm --recursive s3://bucket-name/portal/logs
$  aws s3 rm --recursive s3://bucket-name/portal/resources
----

== Examples

[#_install_portal_eg]
=== Plain RAID installation over PostgreSQL and external S3

Create a common configuration file to use across all installations, lets called it `myconf.yaml`
[source,yaml]

----
deployment:
  base-dir: s3://mybucket/example
storage:
  type: s3
  s3:
    endpoint: http://localhost:9001
    credentials-type: static
    access-key-id: myaccesskey
    secret-access-key: mysecretkey
----

==== Create and upload portal setup configurations

Upload portal config into setup directory.
Example:

[source,xml]
----
<?xml version="1.0" encoding="UTF-8"?>
<ic:instance-config xmlns:ic="urn:wedo:infra:instance-config-2.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
	<global>
		<string name="DbHost">postgres</string>
		<int name="DbPort">5432</int>
		<string name="DbDatabase">postgres</string>
		<string name="DbRdbms">Postgresql</string>
        <string name="DbUsersDbaUser">postgres</string>
        <string name="DbUsersDbaPwd">postgres</string>
        <string name="DbUsersAppUser">PORTALAPP</string>
        <string name="DbUsersAppPwd">PORTALAPP</string>
        <string name="DbUsersAdmUser">PORTALADM</string>
		<string name="DbUsersAdmPwd">PORTALADM</string>
		<string name="CompanyWebIdentifier">mobileum.com</string>
		<string name="ServerAdminPwd">Password1</string>
		<string name="ServerInternalPwd">Password1</string>
        <string name="DbTablespacesSmallData">PG_DEFAULT</string>
        <string name="DbTablespacesMediumData">PG_DEFAULT</string>
        <string name="DbTablespacesLargeData">PG_DEFAULT</string>
        <string name="DbTablespacesLOB">PG_DEFAULT</string>
        <string name="DbTablespacesSmallIndex">PG_DEFAULT</string>
        <string name="DbTablespacesMediumIndex">PG_DEFAULT</string>
        <string name="DbTablespacesLargeIndex">PG_DEFAULT</string>
        <string name="DbTablespacesDefault">PG_DEFAULT</string>
		<string name="DbTablespacesTemp">PG_DEFAULT</string>
        <int name="WebServerPort">8080</int> <!-- Can't be changed -->
		<int name="WebServerShutdownPort">8081</int> <!-- Can't be changed -->
		<string name="ServerMemoryOptions">-Xms128M -Xmx1024M -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:MaxPermSize=256m</string>
        <composite name="ServerHost">${env.DEPLOYMENT_NAME}</composite>
	</global>
</ic:instance-config>

----

[source,shell]
----
aws s3 cp product-config.xml s3://mybucket/example/portal/setup/
----

Get a product license and upload it into setup directory.

[source,shell]
----
$ aws s3 cp license.lic s3://mybucket/example/portal/setup/
----

==== Create and upload portal startup configurations

Upload portal config and license into startup directory.

[source,shell]
----
aws s3 cp product-config.xml s3://mybucket/example/portal/startup/
aws s3 cp license.lic s3://mybucket/example/portal/startup/
----

==== Create and upload RAFM setup configurations

Upload rafm config into setup directory.
Example:

[source,xml]
----
<?xml version="1.0" encoding="UTF-8"?>
<ic:instance-config xmlns:ic="urn:wedo:infra:instance-config-2.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
  <global>
    <string name="DbHost">postgres</string>
    <int name="DbPort">5432</int>
    <string name="DbDatabase">postgres</string>
    <string name="DbRdbms">Postgresql</string>
    <string name="DbUsersDbaUser">postgres</string>
    <string name="DbUsersDbaPwd">postgres</string>
    <string name="DbUsersAdmUser">RAFMADM</string>
    <string name="DbUsersAdmPwd">RAFMADM</string>
    <string name="DbUsersAppUser">RAFMAPP</string>
    <string name="DbUsersAppPwd">RAFMAPP</string>
    <string name="DbUsersDatUser">RAFMDAT</string>
    <string name="DbUsersDatPwd">RAFMDAT</string>
    <string name="DbTablespacesSmallData">PG_DEFAULT</string>
    <string name="DbTablespacesMediumData">PG_DEFAULT</string>
    <string name="DbTablespacesLargeData">PG_DEFAULT</string>
    <string name="DbTablespacesLOB">PG_DEFAULT</string>
    <string name="DbTablespacesSmallIndex">PG_DEFAULT</string>
    <string name="DbTablespacesMediumIndex">PG_DEFAULT</string>
    <string name="DbTablespacesLargeIndex">PG_DEFAULT</string>
    <string name="DbTablespacesDefault">PG_DEFAULT</string>
    <string name="DbTablespacesTemp">PG_DEFAULT</string>
    <string name="PortalDbUsersAdmUser">PORTALRAFM_ADM</string>
    <string name="PortalDbUsersAdmPwd">PORTALRAFM_ADM</string>
    <string name="PortalDbUsersAppUser">PORTALRAFM_APP</string>
    <string name="PortalDbUsersAppPwd">PORTALRAFM_APP</string>
    <string name="ServerAdminUser">adm</string>
    <string name="ServerAdminPwd">Password1</string>
    <string name="InternalServerCommunicationUser">adm</string>
    <string name="InternalServerCommunicationPwd">Password1</string>
    <string name="ServerMemoryOptions">-Xms128M -Xmx1024M -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:-DisableExplicitGC -XX:MaxPermSize=256m </string>
    <string name="MailServer">mailservertochange</string>
    <int name="MailServerPort">25</int>
    <string name="MailServerUser">mailusertochange</string>
    <string name="MailServerPassword">mailpasswordtochange$</string>
    <int name="WebServerPort">8080</int> <!--cannot be changed -->
    <boolean name="EnablePartitioning">true</boolean>
    <composite name="AreaPublicName">${env.DEPLOYMENT_NAME}</composite>
    <composite name="PortletCategory">${env.DEPLOYMENT_NAME}</composite>
    <composite name="ServerHost">${env.DEPLOYMENT_NAME}</composite>
    <string name="WebServerHost">portalServiceName</string>
    <composite name="ServerPort">40353</composite>
  </global>
</ic:instance-config>
----

[source,shell]
----
aws s3 cp product-config.xml s3://mybucket/example/rafm/setup/
----

Get a product license and upload it into setup directory.

[source,shell]
----
$ aws s3 cp license.lic s3://mybucket/example/rafm/setup
----

==== Create and upload RAFM startup configurations

Upload rafm config and license into startup directory.

[source,bash]
----
aws s3 cp product-config.xml s3://mybucket/example/rafm/startup/
aws s3 cp license.lic s3://mybucket/example/rafm/startup/
----

==== Install portal

[source,bash]
----
helm install portal \
  --set deployment.name=portal \
  --set deployment.setup-path=setup \
  --set deployment.startup-path=startup \
  -f myconf.yaml \
  --timeout 30m \
  <PATH_TO_BUNDLE>/charts/
----

Once deployment is ready, access portal in http://localhost:18080, execute:

[source,shell]
----
$ kubectl -n awsdemo port-forward service/portaldemo 18080:8080 --address 0.0.0.0
----

==== Install RAFM

[source,bash]
----
helm install rafm \
  --set deployment.name=rafm \
  --set deployment.setup-path=setup \
  --set deployment.startup-path=startup \
  --set deployment.upload-target=portal \
  -f myconf.yaml \
  --timeout 30m \
  <PATH_TO_BUNDLE>/charts/
----

== Annex

=== Arguments

Please note that the following attributes must not be used:

* PathToPortalInstance


==== Complete RAID argument list
:leveloffset: +1

include::arguments.adoc[]

:leveloffset: -1
